{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Web Scraping and Word Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this module, we will use Natural Language ToolKit along with several other popular Python packages to build a data science pipeline to plot frequency histograms of words in html novels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, you will need a Python installation (3.6.3 or later is recommended).\n",
    "```\n",
    "$ python --version\n",
    "3.6.3\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If needed, run these command inside the terminal to install the packages: \n",
    "\n",
    "```\n",
    "$ pip install beautifulsoup4\n",
    "$ pip install matplotlib\n",
    "$ pip install nltk\n",
    "$ pip install requests\n",
    "$ pip install seaborn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get some data\n",
    "Where do we get data?  That's easy...data is everywhere.  We can import files (csv, xlsx, txt), pull from APIs (usually as JSON), or obtain raw HTML.  For this example, we will use the freely available online at Project Gutenberg.\n",
    "\n",
    "Here are several links to well known HTML books:\n",
    "- 'https://www.gutenberg.org/files/514/514-h/514-h.htm' # Little Women\n",
    "- 'https://www.gutenberg.org/files/42671/42671-h/42671-h.htm' # Pride & Prejudice\n",
    "- 'https://www.gutenberg.org/files/203/203-h/203-h.htm' # Uncle Tom's Cabin\n",
    "- 'https://www.gutenberg.org/files/205/205-h/205-h.htm' # Walden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store url\n",
    "url = 'https://www.gutenberg.org/files/42671/42671-h/42671-h.htm'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to fetch the HTML file.  To do this, we will use a popular package known as ```requests```.  If you are familiar with http requests, we will be submitting a ```GET``` request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the request and check object type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```type``` command outputs the datatype.  Here we are getting a ```Response`` object.\n",
    "\n",
    "The following commands extract and outputs the raw HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract HTML from Response object and print\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangle the data\n",
    "\n",
    "**Tag soup** refers to unstructured (or malformed) HTML code.  The package ```BeautifulSoup``` allows you to easily interact with this code.\n",
    "\n",
    "Because we are in Lousiana, let's refer to our HTML soup as 'gumbo'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a BeautifulSoup object from the HTML\n",
    "gumbo = BeautifulSoup(html, 'html5lib')\n",
    "type(gumbo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our ```gumbo``` object, we can extract some information such as title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get title as string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find the hyperlinks within a page (< a > tags):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hyperlinks from gumbo and check out first several\n",
    "gumbo.findAll('a')[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    For this project, we want the text from the ```gumbo``` object.  Luckily, there is a ```.get_text()``` method for doing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the text out of the gumbo and print it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost there!  While we have the text of the novel, it still contains some metadata.  Since the metadata is minimal and will not influence our findings, let's move forward witht he project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Words\n",
    "Next, we will use ```nltk``` tokenize text and remove stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regex in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sentence\n",
    "sentence = 'peter piper pick a peck of pickled peppers'\n",
    "\n",
    "# Define regex\n",
    "ps = 'p\\w+'\n",
    "\n",
    "# Find all words in sentence that match the regex and print them\n",
    "re.findall(ps, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all words and print them\n",
    "re.findall('\\w+', sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do something similar with the ```text``` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all words in Moby Dick and print several\n",
    "\n",
    "# tokens = nltk.word_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost there!  At this point, words that start with a capital letter will be counted a separate instance.  To handle this issue, make all of the words lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize new list\n",
    "words = []\n",
    "\n",
    "# Loop through list tokens and make lower case\n",
    "\n",
    "\n",
    "# Print several items from list as sanity check\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words provide no real insights so let's remove them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get English stopwords and print some of them\n",
    "sw = nltk.corpus.stopwords.words('english')\n",
    "sw[:5]\n",
    "\n",
    "# If you encounter an error, run the command below.\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize new list\n",
    "words_ns = []\n",
    "\n",
    "# Add to words_ns all words that are in words but not in sw\n",
    "\n",
    "\n",
    "# Print several list items as sanity check\n",
    "words_ns[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering the question\n",
    "We started this project wanting to know the most frequently used words in a novel.  An easy manner to answer this question is to create a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figures inline and set visualization style\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "# Create freq dist and plot\n",
    "freqdist1 = nltk.FreqDist(words_ns)\n",
    "freqdist1.plot(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Create a reusable function\n",
    "\n",
    "There are hundreds of novels on Project Gutenbergso it makes sense to write a function that does utilizes our code from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_freq(url, num = 25):\n",
    "    \"\"\"Takes a url & frequency and plots the word distribution\"\"\"\n",
    "    # Make the request and check object type\n",
    "    r = requests.get(url)\n",
    "    # Extract HTML from Response object and print\n",
    "    html = r.text\n",
    "    # Create a BeautifulSoup object from the HTML\n",
    "    gumbo = BeautifulSoup(html, \"html5lib\")\n",
    "    # Get the text out of the soup and print it\n",
    "    text = gumbo.get_text()\n",
    "    # Create tokens\n",
    "    tokens = re.findall('\\w+', text)\n",
    "    # Initialize new list\n",
    "    words = []\n",
    "    # Loop through list tokens and make lower case\n",
    "    for word in tokens:\n",
    "        words.append(word.lower())\n",
    "    # Get English stopwords and print some of them\n",
    "    sw = nltk.corpus.stopwords.words('english')\n",
    "    # Initialize new list\n",
    "    words_ns = []\n",
    "    # Add to words_ns all words that are in words but not in sw\n",
    "    for word in words:\n",
    "        if word not in sw:\n",
    "            words_ns.append(word)\n",
    "    # Create freq dist and plot\n",
    "    freqdist1 = nltk.FreqDist(words_ns)\n",
    "    freqdist1.plot(num)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_freq('https://www.gutenberg.org/files/514/514-h/514-h.htm', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "What have we learned?  You now have the foundation for 'scraping' HTML data from a website, extracting data, manipulating text, and plotting output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
